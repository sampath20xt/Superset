import os
import chromadb  # ChromaDB as a temporary vector store
import numpy as np
from sentence_transformers import SentenceTransformer
import vertexai
from vertexai.preview.generative_models import GenerativeModel, HarmCategory, HarmBlockThreshold

# Initialize Sentence Transformer Model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Initialize Vertex AI
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "email-extraction-381718-3f73208ce3b71.json"
vertexai.init(project="fleetenable", location="us-central1")
generative_multimodal_model = GenerativeModel("gemini-1.5-pro-001")

# Initialize an In-Memory ChromaDB Collection (Temporary VectorDB)
chroma_client = chromadb.PersistentClient(path=":memory:")  # Temporary storage
vector_collection = chroma_client.get_or_create_collection("temp_vectors")

# Load and preprocess the txt file
def load_data(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        raw_text = f.readlines()

    # Convert to structured format if needed
    data = [line.strip() for line in raw_text if line.strip()]
    return data

# Generate embeddings and store them in ChromaDB
def store_embeddings(text_list):
    vector_collection.delete(where={})  # Clear previous embeddings (if any)

    for idx, text in enumerate(text_list):
        embedding = embedding_model.encode([text], convert_to_numpy=True)[0].tolist()
        vector_collection.add(
            ids=[str(idx)],  # Unique identifier for each vector
            embeddings=[embedding],
            metadatas=[{"text": text}]
        )

# Perform vector search in ChromaDB
def search_similar_texts(query, k=3):
    query_embedding = embedding_model.encode([query], convert_to_numpy=True)[0].tolist()
    
    # Search using cosine similarity
    results = vector_collection.query(
        query_embeddings=[query_embedding],
        n_results=k
    )
    
    relevant_texts = [item["text"] for item in results["metadatas"][0]]
    return relevant_texts

# Query Gemini AI with retrieved context
def query_gemini(context, user_query):
    prompt = f"""
    You are an advanced AI system designed to extract relevant insights from structured and unstructured data.
    Use the retrieved information below to accurately answer the user's query.

    Retrieved Information:
    {context}

    User Query:
    {user_query}

    Instructions:
    Your response should be based only on the retrieved information and avoid adding any assumptions.
    If the retrieved information does not explicitly contain an answer, state that the requested information is not found.
    Provide concise, direct, and structured answers.

    Answer:
    """
    generation_config = {
        "max_output_tokens": 8192,
        "temperature": 0.2,
        "top_p": 0.3,
    }

    safety_settings = {
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    }

    response = generative_multimodal_model.generate_content(prompt, generation_config=generation_config,
                                                            safety_settings=safety_settings)

    return response.text

# Main Execution
if __name__ == "__main__":
    while True:
        file_path = "Data/output.csv"
        text_data = load_data(file_path)

        # Generate and store embeddings in ChromaDB
        store_embeddings(text_data)

        # Take user input and perform retrieval
        user_query = input("\nEnter your query (or type 'exit' to quit): ")
        if user_query.lower() == "exit":
            print("Exiting...")
            break

        relevant_texts = search_similar_texts(user_query)

        context = "\n".join(relevant_texts)

        # Query Gemini AI and display the result
        response = query_gemini(context, user_query)
        print("\nGemini Response:\n", response)
