import faiss
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from google.cloud import aiplatform
import json

# Load the sentence-transformers model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Load and preprocess the txt file
def load_data(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        raw_text = f.readlines()

    # Convert to structured format if needed
    data = [line.strip() for line in raw_text if line.strip()]
    return data

# Generate embeddings for each text chunk
def generate_embeddings(text_list):
    embeddings = embedding_model.encode(text_list, convert_to_numpy=True)
    return embeddings

# Create a new FAISS index (temporary storage)
def create_faiss_index(embeddings):
    dim = embeddings.shape[1]  # Dimension of embeddings
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index

# Perform vector search
def search_similar_texts(query, text_list, index, k=3):
    query_embedding = embedding_model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_embedding, k)

    results = [text_list[i] for i in indices[0] if i < len(text_list)]
    return results

# Query Gemini AI with retrieved context
def query_gemini(prompt, project_id, location="us-central1", model_name="gemini-pro"):
    aiplatform.init(project=project_id, location=location)

    model = aiplatform.generation_models.TextGenerationModel.from_pretrained(model_name)

    response = model.predict(
        prompt=prompt,
        temperature=0.2,
        max_output_tokens=300
    )

    return response.text

# Main execution
if __name__ == "__main__":
    while True:  # Allow multiple queries while keeping embeddings temporary
        # Load the new data each time
        file_path = "data.txt"  # Replace with your file path
        text_data = load_data(file_path)

        # Generate embeddings for new data
        embeddings = generate_embeddings(text_data)

        # Create a temporary FAISS index (previous embeddings are lost)
        index = create_faiss_index(embeddings)

        # Take user input and perform retrieval
        user_query = input("\nEnter your query (or type 'exit' to quit): ")
        if user_query.lower() == "exit":
            print("Exiting...")
            break

        relevant_texts = search_similar_texts(user_query, text_data, index)

        # Prepare the prompt for Gemini AI
        context = "\n".join(relevant_texts)
        final_prompt = f"Based on the following retrieved information, answer the question:\n\n{context}\n\nQuestion: {user_query}\n\nAnswer:"

        # Query Gemini AI
        project_id = "your-gcp-project-id"  # Replace with your GCP project ID
        response = query_gemini(final_prompt, project_id)

        # Display the answer
        print("\nGemini Response:\n", response)

        # **Embeddings are deleted here when the loop repeats**
